{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a38c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import alris_functions2\n",
    "reload(alris_functions2)\n",
    "from alris_functions2 import shift_atoms, transform_list_hkl_p63_p65, get_structure_factors , atom_position_list\n",
    "from itertools import chain\n",
    "from matplotlib.markers import MarkerStyle\n",
    "\n",
    "tf.keras.utils.set_random_seed(1)\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg' # make figures appears in .svg style\n",
    "\n",
    "num_threads = 2  \n",
    "\n",
    "# Configure TensorFlow to use multiple threads\n",
    "tf.config.threading.set_intra_op_parallelism_threads(num_threads)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(num_threads)\n",
    "\n",
    "print(f\"Using {num_threads} threads for intra-op and inter-op parallelism.\")\n",
    "\n",
    "\n",
    "\n",
    "def fun_tf(hkl_list, pars, matrix):\n",
    "    \"\"\"\n",
    "    Fast computation of structure factors with parameter-dependent structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get modified structure\n",
    "    pars_tensor = tf.stack(pars)  # shape (params,)\n",
    "\n",
    "    atom_shift_list = shift_atoms(matrix , (pars_tensor))\n",
    "    atom_shift_list = atom_shift_list[:,0]\n",
    "    atom_shift_list = tf.unstack(atom_shift_list)\n",
    "\n",
    "    modified_struct = atom_position_list(*atom_shift_list)\n",
    "\n",
    "    hkl_list = transform_list_hkl_p63_p65(hkl_list)\n",
    "\n",
    "    sf_hkl = get_structure_factors(hkl_list, modified_struct)\n",
    "    intensity = (abs(sf_hkl)) ** 2\n",
    "    w = tf.constant(0.00032001553565274784, dtype=tf.float32)  # Debye-Waller factor \n",
    "    qnorms = tf.norm(tf.cast(hkl_list, tf.float32), axis=1)\n",
    "    intensity = intensity * tf.exp(- w* qnorms ** 2)  # Apply Debye-Waller factor\n",
    "\n",
    "    intensity = intensity / tf.reduce_sum(intensity) * 60\n",
    "    return intensity\n",
    "\n",
    "\n",
    "class FunAsLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, matrix , max_mode_amps,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_mode_amps = max_mode_amps\n",
    "        self.matrix = matrix\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.param = self.add_weight(name='param', shape=(1290,), initializer=tf.keras.initializers.GlorotNormal(seed=1), trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply tanh to ensure parameters stay within the [-1, 1] range then multiply by max_mode_amps so each parameter is scaled corresponding to the element in max_mode_amps\n",
    "        pretransform = tf.tanh(self.param)\n",
    "        transformed_params = pretransform * self.max_mode_amps  # Scale parameters\n",
    "\n",
    "        output = fun_tf(inputs, (transformed_params) , self.matrix)\n",
    "        return tf.reshape(output , [-1])  # Ensure output is 1D\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "# R-Score based on intensity\n",
    "class RFactorLoss(tf.keras.losses.Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        return tf.reduce_sum(tf.abs(y_true - y_pred)) / tf.reduce_sum(y_true)\n",
    "\"\"\"\n",
    "    \n",
    "# mean squared error\n",
    "class PerSampleMSE(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__(reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        squared_error = tf.square(y_true - y_pred)\n",
    "        per_sample_mse = tf.reduce_mean(squared_error, axis=-1)\n",
    "        return per_sample_mse  # shape (batch_size,)\n",
    "  \n",
    "# Define the custom metric function\n",
    "def r_factor_metric(y_true, y_pred):\n",
    "    labels = y_true\n",
    "    return tf.reduce_sum(tf.abs(labels - y_pred)) / tf.reduce_sum(labels)\n",
    "\n",
    "\n",
    "def make_sample_weights(experimental_data):\n",
    "    labels = experimental_data[\"intensity_exp\"].tolist()\n",
    "    labels = labels / np.sum(labels) * 60 #Normalize labels\n",
    "    vol_err = experimental_data[\"intensity_exp_err\"].tolist()\n",
    "\n",
    "    labels_err = []\n",
    "\n",
    "    for label, err in zip(labels, vol_err):\n",
    "        if label == 0:\n",
    "            labels_err.append(10)  # Assign a high error for zero labels\n",
    "        else:\n",
    "            labels_err.append(1/(label))  # Inverse error for each label\n",
    "\n",
    "    labels_err = tf.convert_to_tensor(labels_err, dtype=tf.float32)\n",
    "    labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "\n",
    "    labels = tf.expand_dims(labels, axis=-1)  # Ensure labels are 2D\n",
    "    labels_err = tf.expand_dims(labels_err, axis=-1)  # Ensure labels_err are 2D\n",
    "\n",
    "    return labels, labels_err\n",
    "\n",
    "def fn_distortion_fitting(labels , labels_err , matrix , features , n_dim , iteration_num):\n",
    "    #initialise the histogram matrix\n",
    "    histogram_matrix = tf.zeros([iteration_num , iteration_num], dtype=tf.float32)\n",
    "    \n",
    "    lr = [5e-1]\n",
    "    best_pars_overall = None\n",
    "    best_rf_overall = np.inf\n",
    "    best_loss_overall = []\n",
    "\n",
    "    for learning_rate in lr:\n",
    "\n",
    "        optim = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        max_mode_amps = tf.constant([5.6,6.36,5.6,11.19,11.19,12.73,11.19,12.73,11.19,11.19,12.73,11.19,9.0,7.91,9.0,7.91,9.0,9.0,9.0,7.91,7.91,9.0,7.91,9.0,9.0,9.0,7.91,9.0,7.91,7.91,7.91,7.91,7.91,9.0,7.91,7.91,7.91,7.91,7.91,7.91,7.91,9.0,7.91,7.91,9.0,7.91,9.0,7.91,9.0,9.0,9.0,7.91,7.91,7.91,7.91,7.91,7.91,9.0,7.91,7.91,5.6,6.36,5.6,5.6,6.36,6.36,5.6,6.36,5.6,5.6,6.36,5.6,5.6,6.36,5.6,5.6,6.36,6.36,5.6,6.36,5.6,6.36,6.36,5.6,6.36,6.36,5.6,6.36,6.36,6.36,6.36,6.36,5.6,5.6,6.36,5.6,5.6,6.36,5.6,5.6,6.36,6.36,5.6,6.36,5.6,8.29,7.91,8.04,8.42,8.04,8.42,0.0,15.59,15.83,15.59,15.83,15.59,18.0,15.59,15.83,15.59,18.0,15.59,15.83,15.59,15.83,15.59,18.0,15.59,15.83,11.73,11.19,11.37,11.91,11.73,11.19,11.37,11.91,11.02,12.73,14.7,12.73,11.73,11.19,11.37,11.91,11.73,11.19,11.37,11.91,11.73,11.19,11.37,11.91,11.02,12.73,14.7,12.73,11.73,11.19,11.37,11.91,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,11.02,11.19,12.73,11.02,11.19,12.73,11.02,12.73,12.73,11.02,11.19,12.73,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,7.79,9.0,9.14,7.91,9.14,7.91,8.29,7.91,9.14,9.58,8.04,8.42,8.29,7.91,8.04,8.42,8.04,8.42,7.79,9.0,9.14,7.91,9.14,7.91,8.29,7.91,8.04,8.42,8.04,8.42,8.29,7.91,9.14,9.58,8.04,8.42,7.79,9.0,9.14,7.91,9.14,7.91,8.29,7.91,8.04,8.42,9.14,9.58,8.29,7.91,8.04,8.42,9.14,9.58,7.79,9.0,10.39,9.0,10.39,9.0,8.29,7.91,8.04,8.42,9.14,9.58,8.29,7.91,8.04,8.42,8.04,8.42,7.79,9.0,9.14,7.91,9.14,7.91,8.29,7.91,9.14,9.58,8.04,8.42,8.29,7.91,8.04,8.42,8.04,8.42,5.6,6.36,5.6,11.19,11.19,12.73,11.19,12.73,11.19,11.19,12.73,11.19,9.0,7.91,9.0,7.91,9.0,9.0,9.0,7.91,7.91,9.0,7.91,9.0,9.0,9.0,7.91,9.0,7.91,7.91,7.91,7.91,7.91,9.0,7.91,7.91,7.91,7.91,7.91,7.91,7.91,9.0,7.91,7.91,9.0,7.91,9.0,7.91,9.0,9.0,9.0,7.91,7.91,7.91,7.91,7.91,7.91,9.0,7.91,7.91,5.6,6.36,5.6,5.6,6.36,6.36,5.6,6.36,5.6,5.6,6.36,5.6,5.6,6.36,5.6,5.6,6.36,6.36,5.6,6.36,5.6,6.36,6.36,5.6,6.36,6.36,5.6,6.36,6.36,6.36,6.36,6.36,5.6,5.6,6.36,5.6,5.6,6.36,5.6,5.6,6.36,6.36,5.6,6.36,5.6,7.79,9.0,9.14,7.91,9.14,7.91,0.0,15.59,15.83,15.59,15.83,15.59,18.0,15.59,15.83,15.59,18.0,15.59,15.83,15.59,15.83,15.59,18.0,15.59,15.83,11.02,12.73,12.92,11.19,11.02,12.73,12.92,11.19,11.02,12.73,14.7,12.73,11.02,12.73,12.92,11.19,11.02,12.73,12.92,11.19,11.02,12.73,12.92,11.19,11.02,12.73,14.7,12.73,11.02,12.73,12.92,11.19,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,11.02,11.19,12.73,11.02,11.19,12.73,11.02,12.73,12.73,11.02,11.19,12.73,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,7.79,9.0,9.14,7.91,9.14,7.91,7.79,9.0,10.39,9.0,9.14,7.91,7.79,9.0,9.14,7.91,9.14,7.91,7.79,9.0,9.14,7.91,9.14,7.91,7.79,9.0,9.14,7.91,9.14,7.91,7.79,9.0,10.39,9.0,9.14,7.91,7.79,9.0,9.14,7.91,9.14,7.91,7.79,9.0,9.14,7.91,10.39,9.0,7.79,9.0,9.14,7.91,10.39,9.0,7.79,9.0,10.39,9.0,10.39,9.0,7.79,9.0,9.14,7.91,10.39,9.0,7.79,9.0,9.14,7.91,9.14,7.91,7.79,9.0,9.14,7.91,9.14,7.91,7.79,9.0,10.39,9.0,9.14,7.91,7.79,9.0,9.14,7.91,9.14,7.91,5.6,6.36,5.6,11.19,11.19,12.73,11.19,12.73,11.19,11.19,12.73,11.19,9.0,7.91,9.0,7.91,9.0,9.0,9.0,7.91,7.91,9.0,7.91,9.0,9.0,9.0,7.91,9.0,7.91,7.91,7.91,7.91,7.91,9.0,7.91,7.91,7.91,7.91,7.91,7.91,7.91,9.0,7.91,7.91,9.0,7.91,9.0,7.91,9.0,9.0,9.0,7.91,7.91,7.91,7.91,7.91,7.91,9.0,7.91,7.91,5.6,6.36,5.6,5.6,6.36,6.36,5.6,6.36,5.6,5.6,6.36,5.6,5.6,6.36,5.6,5.6,6.36,6.36,5.6,6.36,5.6,6.36,6.36,5.6,6.36,6.36,5.6,6.36,6.36,6.36,6.36,6.36,5.6,5.6,6.36,5.6,5.6,6.36,5.6,5.6,6.36,6.36,5.6,6.36,5.6,8.29,7.91,8.04,8.42,8.04,8.42,0.0,15.59,15.83,15.59,15.83,15.59,18.0,15.59,15.83,15.59,18.0,15.59,15.83,15.59,15.83,15.59,18.0,15.59,15.83,11.02,12.73,12.92,11.19,11.02,12.73,12.92,11.19,11.02,12.73,14.7,12.73,11.02,12.73,12.92,11.19,11.73,11.19,11.37,11.91,11.73,11.19,11.37,11.91,11.02,12.73,14.7,12.73,11.73,11.19,11.37,11.91,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,11.02,11.19,12.73,11.02,11.19,12.73,11.02,12.73,12.73,11.02,11.19,12.73,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,8.29,7.91,8.04,8.42,8.04,8.42,8.29,7.91,9.14,9.58,8.04,8.42,8.29,7.91,8.04,8.42,8.04,8.42,8.29,7.91,8.04,8.42,8.04,8.42,8.29,7.91,8.04,8.42,8.04,8.42,8.29,7.91,9.14,9.58,8.04,8.42,8.29,7.91,8.04,8.42,8.04,8.42,7.79,9.0,9.14,7.91,10.39,9.0,7.79,9.0,9.14,7.91,10.39,9.0,7.79,9.0,10.39,9.0,10.39,9.0,7.79,9.0,9.14,7.91,10.39,9.0,8.29,7.91,8.04,8.42,8.04,8.42,8.29,7.91,8.04,8.42,8.04,8.42,8.29,7.91,9.14,9.58,8.04,8.42,8.29,7.91,8.04,8.42,8.04,8.42,8.29,7.91,8.04,8.42,8.04,8.42,0.0,15.59,15.83,15.59,15.83,15.59,18.0,15.59,15.83,15.59,18.0,15.59,15.83,15.59,15.83,15.59,18.0,15.59,15.83,11.73,11.19,11.37,11.91,11.73,11.19,11.37,11.91,11.02,12.73,14.7,12.73,11.73,11.19,11.37,11.91,11.02,12.73,12.92,11.19,11.02,12.73,12.92,11.19,11.02,12.73,14.7,12.73,11.02,12.73,12.92,11.19,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,11.02,11.19,12.73,11.02,11.19,12.73,11.02,12.73,12.73,11.02,11.19,12.73,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,8.29,7.91,8.04,8.42,8.04,8.42,7.79,9.0,10.39,9.0,9.14,7.91,8.29,7.91,8.04,8.42,8.04,8.42,8.29,7.91,8.04,8.42,8.04,8.42,8.29,7.91,8.04,8.42,8.04,8.42,7.79,9.0,10.39,9.0,9.14,7.91,8.29,7.91,8.04,8.42,8.04,8.42,8.29,7.91,8.04,8.42,9.14,9.58,8.29,7.91,8.04,8.42,9.14,9.58,7.79,9.0,10.39,9.0,10.39,9.0,8.29,7.91,8.04,8.42,9.14,9.58,8.29,7.91,8.04,8.42,8.04,8.42,8.29,7.91,8.04,8.42,8.04,8.42,7.79,9.0,10.39,9.0,9.14,7.91,8.29,7.91,8.04,8.42,8.04,8.42,7.79,9.0,9.14,7.91,9.14,7.91,0.0,15.59,15.83,15.59,15.83,15.59,18.0,15.59,15.83,15.59,18.0,15.59,15.83,15.59,15.83,15.59,18.0,15.59,15.83,11.02,12.73,12.92,11.19,11.02,12.73,12.92,11.19,11.02,12.73,14.7,12.73,11.02,12.73,12.92,11.19,11.02,12.73,12.92,11.19,11.02,12.73,12.92,11.19,11.02,12.73,14.7,12.73,11.02,12.73,12.92,11.19,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,11.02,11.19,12.73,11.02,11.19,12.73,11.02,12.73,12.73,11.02,11.19,12.73,11.02,11.19,11.19,11.02,11.19,11.19,11.02,12.73,11.19,11.02,11.19,11.19,7.79,9.0,9.14,7.91,9.14,7.91,7.79,9.0,10.39,9.0,9.14,7.91,7.79,9.0,9.14,7.91,9.14,7.91,7.79,9.0,9.14,7.91,9.14,7.91,7.79,9.0,9.14,7.91,9.14,7.91,7.79,9.0,10.39,9.0,9.14,7.91,7.79,9.0,9.14,7.91,9.14,7.91,7.79,9.0,9.14,7.91,10.39,9.0,7.79,9.0,9.14,7.91,10.39,9.0,7.79,9.0,10.39,9.0,10.39,9.0,7.79,9.0,9.14,7.91,10.39,9.0,7.79,9.0,9.14,7.91,9.14,7.91,7.79,9.0,9.14,7.91,9.14,7.91,7.79,9.0,10.39,9.0,9.14,7.91,7.79,9.0,9.14,7.91,9.14,7.91])\n",
    "        n_epochs = 100\n",
    "        histories = []\n",
    "        n_iter = iteration_num\n",
    "\n",
    "        min_loss = np.inf\n",
    "        best_pars = None\n",
    "\n",
    "        # List to store the loss values for each epoch\n",
    "        all_losses = []\n",
    "\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            # Create the model\n",
    "            inputs = tf.keras.Input(shape=(n_dim,))\n",
    "            outputs = FunAsLayer(matrix , max_mode_amps)(inputs)\n",
    "            model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "            # Compile the model with the custom loss function and metric\n",
    "            model.compile(\n",
    "                optimizer=optim,\n",
    "                loss= 'mse', # MSE_weighted() if using errors\n",
    "                metrics=[r_factor_metric],\n",
    "                run_eagerly=False,  # Set to True for debugging, False for performance\n",
    "\n",
    "            )\n",
    "            \n",
    "            history = model.fit(\n",
    "            x=features,\n",
    "            y=labels,  # replace with combined_labels if using errors\n",
    "            batch_size = features.shape[0], # Use a smaller batch size features.shape[0]\n",
    "            epochs=n_epochs,\n",
    "            verbose='auto',\n",
    "            shuffle=True, # not sure whether this matters\n",
    "            # callbacks=[cb]\n",
    "            sample_weight=labels_err  # Use sample weights if you have errors\n",
    "            )\n",
    "\n",
    "            histories.append(history)\n",
    "            all_losses.append(history.history['loss'])\n",
    "            # Check final loss\n",
    "            final_loss = history.history['loss'][-1]\n",
    "            print(model.layers[-1].get_weights()[0].shape)\n",
    "            curren_model_pars = max_mode_amps * tf.tanh(model.layers[-1].get_weights()[0])\n",
    "            print(f\"Final loss: {final_loss:.3e}\")\n",
    "            '''\n",
    "            print(f\"Best parameters for iteration {i+1}:\")\n",
    "            for j, par in enumerate(curren_model_pars):\n",
    "                print(f\"Parameter {j+1}: {par.numpy():.4f}\")\n",
    "            '''\n",
    "            #open the file and write the parameters\n",
    "            with open('C:/Users/User/Desktop/uzh_intern/CrystalClearFit/alrisDistortionFit/PBCO/new_PBCO_fit/fitted_data/histogram_pars.txt', 'a') as f:\n",
    "                for idx , par in enumerate(curren_model_pars):\n",
    "                    f.write(str(par.numpy()) + '\\n')\n",
    "\n",
    "            if final_loss < min_loss:\n",
    "                # Update best model parameters\n",
    "                best_model_pars = max_mode_amps * tf.tanh(model.layers[-1].get_weights()[0])\n",
    "                min_loss = final_loss\n",
    "                rf = r_factor_metric(labels, fun_tf(features, best_model_pars , matrix))\n",
    "                print(f\"Iteration {i+1} - New best loss: {min_loss:.3e} (R-factor: {rf:.3e})\")\n",
    "\n",
    "        if min_loss < best_rf_overall:\n",
    "            best_rf_overall = min_loss\n",
    "            best_pars_overall = best_model_pars\n",
    "            best_loss_overall = all_losses\n",
    "\n",
    "        # Plotting the loss values\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot the loss values for each iteration\n",
    "        for i, loss_values in enumerate(all_losses):\n",
    "            plt.plot(loss_values, label=f'Iteration {i+1}')\n",
    "\n",
    "\n",
    "        return histogram_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load experimental data\n",
    "    experimental_data = pd.read_csv('C:/Users/User/Desktop/uzh_intern/CrystalClearFit/alrisDistortionFit/PBCO/raw_data/combined_peaks.csv')\n",
    "    matrix = np.loadtxt('C:/Users/User/Desktop/uzh_intern/CrystalClearFit/alrisDistortionFit/PBCO/new_PBCO_fit/matrix.txt', dtype=np.float32)\n",
    "\n",
    "    n_features = experimental_data.shape[0]\n",
    "    n_dim = 3\n",
    "\n",
    "    hkl_list = experimental_data[[\"h\", \"k\", \"l\"]].values.tolist()\n",
    "    hkl_list = tf.convert_to_tensor(hkl_list, dtype=tf.float32)\n",
    "\n",
    "    matrix = tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
    "\n",
    "    labels, labels_err = make_sample_weights(experimental_data)\n",
    "\n",
    "    histogram_matrix = fn_distortion_fitting(labels, labels_err, matrix, hkl_list, n_dim, iteration_num=1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
